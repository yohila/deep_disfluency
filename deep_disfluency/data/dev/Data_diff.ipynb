{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See how the old switchboard trainin, test and heldout data are different to the current one\n",
    "# to see if that accounts for the drop in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "swbd_old = \"/media/dsg-labuser/NO_NAME/IS_15_swbd_data/data/switchboard/swbd_train_data.csv\"\n",
    "swbd_new = \"../disfluency_detection/switchboard/swbd_disf_train_1_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the old methods\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_old(lol, seed):\n",
    "    '''\n",
    "    lol :: list of list as input\n",
    "    seed :: seed the shuffling\n",
    "\n",
    "    shuffle inplace each list in the same order\n",
    "    '''\n",
    "    for l in lol:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(l)\n",
    "\n",
    "def minibatch_old(l, bs):\n",
    "    '''\n",
    "    l :: list of word idxs\n",
    "    return a list of minibatches of indexes\n",
    "    which size is equal to bs\n",
    "    border cases are treated as follow:\n",
    "    eg: [0,1,2,3] and bs = 3\n",
    "    will output:\n",
    "    [[0],[0,1],[0,1,2],[1,2,3]]\n",
    "    '''\n",
    "    out  = [l[:i] for i in xrange(1, min(bs,len(l)+1) )]\n",
    "    out += [l[i-bs:i] for i in xrange(bs,len(l)+1) ]\n",
    "    assert len(l) == len(out)\n",
    "    return out\n",
    "\n",
    "def indicesFromLength_old(sentenceLength,bs,totalSize):\n",
    "    '''\n",
    "    return a list of indexes pairs (start/stop) for each word\n",
    "    max difference between start and stop equal to bs\n",
    "    border cases are treated as follow:\n",
    "    eg: sentenceLength=4 and bs = 3\n",
    "    will output:\n",
    "    [[0,0],[0,1],[0,2],[1,3]]\n",
    "    '''\n",
    "    l = map(lambda x: totalSize+x ,\\\n",
    "                xrange(sentenceLength))\n",
    "    out = []\n",
    "    for i in xrange(0, min(bs,len(l)) ):\n",
    "        out.append([l[0],l[i]]) \n",
    "    for i in xrange(bs+1,len(l)+1):\n",
    "        out.append([l[i-bs],l[i-1]])\n",
    "    assert len(l) == sentenceLength\n",
    "    return out\n",
    "\n",
    "def contextwin_old(l, win):\n",
    "    '''\n",
    "    win :: int corresponding to the size of the window\n",
    "    given a list of indexes composing a sentence\n",
    "    it will return a list of list of indexes corresponding\n",
    "    to context windows surrounding each word in the sentence\n",
    "    '''\n",
    "    assert (win % 2) == 1\n",
    "    assert win >=1\n",
    "    l = list(l)\n",
    "\n",
    "    lpadded = win/2 * [-1] + l + win/2 * [-1]\n",
    "    out = [ lpadded[i:i+win] for i in range(len(l)) ]\n",
    "\n",
    "    assert len(out) == len(l)\n",
    "    return out\n",
    "\n",
    "def contextwinbackwards_old(l, win):\n",
    "    '''\n",
    "    Same as contextwin except only backwards context (i.e. like an n-gram model)\n",
    "    '''\n",
    "    #assert (win % 2) == 1\n",
    "    assert win >=1\n",
    "    l = list(l)\n",
    "    lpadded = (win-1) * [-1] + l\n",
    "    out = [ lpadded[i:i+win] for i in range(len(l)) ]\n",
    "\n",
    "    assert len(out) == len(l)\n",
    "    return out\n",
    "\n",
    "def corpusToIndexedMatrix_old(my_array_list, win, bs):\n",
    "    '''\n",
    "    Returns a matrix of contextwins for a list of utterances of dimensions win * n_words_in_corpus (i.e. total length of all arrays in my_array_list)\n",
    "    and corresponding matrix of indexes (of just start/stop for each one) so 2 * n_words_in_corpus\n",
    "    of where to access these, using bs (backprop distance) as the limiting history size\n",
    "    '''\n",
    "    sentences = [] # a list (of arrays, or lists?), returned as matrix\n",
    "    indices = [] #a list of index pairs (arrays?), returned as matrix\n",
    "    totalSize = 0\n",
    "    for sentence in my_array_list:\n",
    "        #print totalSize\n",
    "        #print sentence\n",
    "        cwords = contextwinbackwards_old(sentence, win) #get list of context windows\n",
    "        cindices = indicesFromLength_old(len(cwords),bs,totalSize)\n",
    "\n",
    "        indices.extend(cindices)\n",
    "        sentences.extend(cwords)\n",
    "        totalSize+=len(cwords)\n",
    "    \n",
    "    return np.matrix(sentences, dtype='int32'), indices\n",
    "\n",
    "\n",
    "import gzip\n",
    "import cPickle\n",
    "import urllib\n",
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "from os.path import isfile\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PREFIX = os.getenv('ATISDATA', '')\n",
    "SWITCHBOARDPREFIX = '/media/dsg-labuser/NO_NAME/IS_15_swbd_data/data/switchboard/'\n",
    "\n",
    "def switchboardfold_old(fold=None, rpMid=False):\n",
    "    if not fold is None:\n",
    "        assert fold in range(10)\n",
    "        ftrain = open(SWITCHBOARDPREFIX + 'FOLD'+str(fold)+'.csv.text')\n",
    "    else:\n",
    "        ftrain = open(SWITCHBOARDPREFIX +'swbd_train_data.csv')\n",
    "        fval = open(SWITCHBOARDPREFIX + 'swbd_heldout_data.csv')\n",
    "        ftest = open(SWITCHBOARDPREFIX + 'swbd_test_data.csv')\n",
    "        fval2 = open(SWITCHBOARDPREFIX + 'swbd_heldout_data.csv') #dummy\n",
    "        ftest2 = open(SWITCHBOARDPREFIX + 'swbd_test_data.csv') #dummy\n",
    "    dict = defaultdict()\n",
    "    dict['words2idx'] = load_word_rep_old(SWITCHBOARDPREFIX +'swbd_word_rep.csv')\n",
    "    dict['pos2idx'] = load_word_rep_old(SWITCHBOARDPREFIX + 'swbd_pos_rep.csv')\n",
    "    if rpMid == True:\n",
    "        dict['labels2idx'] = load_tags_old(SWITCHBOARDPREFIX +'swbd_tags_rpmid.csv')\n",
    "    else:\n",
    "        dict['labels2idx'] = load_tags_old(SWITCHBOARDPREFIX +'swbd_tags.csv')\n",
    "\n",
    "\n",
    "    #also have a traindict which has only the tags it can be trained on\n",
    "    train_dict = defaultdict()\n",
    "    if rpMid == True:\n",
    "         train_dict['labels2idx'] = load_tags_old(SWITCHBOARDPREFIX +'swbd_train_tags_rpmid.csv')\n",
    "    else:\n",
    "        train_dict['labels2idx'] = load_tags_old(SWITCHBOARDPREFIX +'swbd_train_tags.csv')\n",
    "    \n",
    "    l = load_data_from_file_old(ftrain, dict['words2idx'], dict['pos2idx'], train_dict['labels2idx'], rpMid=rpMid)\n",
    "    l1 = load_data_from_file_old(fval, dict['words2idx'], dict['pos2idx'], train_dict['labels2idx'], rpMid=rpMid)\n",
    "    l2 = load_data_from_file_old(ftest, dict['words2idx'], dict['pos2idx'], train_dict['labels2idx'], rpMid=rpMid)\n",
    "    l1_all = load_data_from_file_old(fval2, dict['words2idx'], dict['pos2idx'], dict['labels2idx'], rpMid=rpMid) #val set with all tags\n",
    "    l2_all = load_data_from_file_old(ftest2, dict['words2idx'], dict['pos2idx'], dict['labels2idx'], rpMid=rpMid) #test set with all tags\n",
    "    \n",
    "    return l,l1,l2,l1_all,l2_all,dict,train_dict\n",
    "\n",
    "def load_word_rep_old(filepath, dimension=None, word_rep_type=\"one_hot\"):\n",
    "    \"\"\"Returns a word_rep_dictionary from word(string) indicating an index by an integer\"\"\"\n",
    "    word_rep_dictionary = None\n",
    "    if word_rep_type == \"one_hot\":\n",
    "        word_rep_dictionary = defaultdict(int) #TODO could use sparse matrices instead?\n",
    "        f = open(filepath)\n",
    "        for line in f:\n",
    "            l = line.split(\",\")\n",
    "            word_rep_dictionary[l[0]] = int(l[1])\n",
    "        f.close()\n",
    "    elif word_rep_type == \"word_freq_count\":\n",
    "        raise NotImplementedError()\n",
    "    elif word_rep_type == \"neural_word\":\n",
    "        raise NotImplementedError()\n",
    "    return word_rep_dictionary\n",
    "\n",
    "def load_tags_old(filepath):\n",
    "    \"\"\"Returns a tag dictionary from word to a n int indicating index by an integer\"\"\"\n",
    "    tag_dictionary = defaultdict(int) #TODO could use sparse matrices instead?\n",
    "    f = open(filepath)\n",
    "    for line in f:\n",
    "        l = line.strip('\\n').split(\",\")\n",
    "        tag_dictionary[l[0]] = int(l[1])\n",
    "    f.close()\n",
    "    return tag_dictionary\n",
    "\n",
    "def load_data_from_file_old(f, word_rep, pos_rep, tags, rpMid=False, n_seq=None):\n",
    "    \"\"\"Loads into a two lists of arrays, one for words (seq), one for tags (targets), both equal length.\"\"\"\n",
    "    print \"loading training data\"\n",
    "    #f = open(filepath)\n",
    "    count_seq = 0\n",
    "    count_step = 0\n",
    "    seq = []\n",
    "    pos_seq = []\n",
    "    targets = []\n",
    "    currentUtt = []\n",
    "    currentPOS = []\n",
    "    currentTags = []\n",
    "    for line in f:\n",
    "        l = line.rstrip(\"\\r\\n\") # should be sequence_number(at first one of sequence) + word + tag\n",
    "        l = l.split('\\t')\n",
    "        if not l[0] == \"\" and not currentUtt == []: #new utterance\n",
    "            count_seq+=1\n",
    "            x = np.asarray(currentUtt)\n",
    "            p = np.asarray(currentPOS)\n",
    "            y = np.asarray(currentTags)\n",
    "            seq.append(x)\n",
    "            pos_seq.append(p)\n",
    "            targets.append(y)\n",
    "            currentUtt = []\n",
    "            currentPOS = []\n",
    "            currentTags = []\n",
    "        if (not n_seq == None) and count_seq >= n_seq: break\n",
    "        w = word_rep.get(l[1])\n",
    "        pos = pos_rep.get(l[2])\n",
    "        tag = tags.get(str(l[len(l)-1])) # NB POS tags in switchboard at l[2]\n",
    "        if tag == None:\n",
    "            if str(l[len(l)-1]) == \"<rpMid/>\" and rpMid==False:\n",
    "                tag = tags.get(\"<f/>\")\n",
    "            elif \"rpMid\" in str(l[len(l)-1]):\n",
    "                tag = tags.get(\"<rm-8/><rpMid/>\")\n",
    "            elif \"rpEndSub\" in str(l[len(l)-1]):\n",
    "                tag = tags.get(\"<rm-8/><rpEndSub/>\")\n",
    "            elif \"rpEndDel\" in str(l[len(l)-1]):\n",
    "                tag = tags.get(\"<rm-8/><rpEndSub/>\")\n",
    "            else:\n",
    "                s = \"No tag in tag dict:\" + str(l[len(l)-1])+\"%%%\"\n",
    "                raw_input(s)\n",
    "                \n",
    "            #print tags\n",
    "        if w == None:\n",
    "            logging.info(\"No word rep for \" + l[1])\n",
    "            #print l[1]\n",
    "            w = word_rep.get(\"<unk>\")\n",
    "        if pos == None:\n",
    "            logging.info(\"No pos rep for \" + l[2])\n",
    "            #print l[2]\n",
    "            pos = pos_rep.get(\"<unk>\")\n",
    "        \n",
    "        currentUtt.append(w) #one-hot encoding\n",
    "        currentPOS.append(pos) #one-hot encoding of POS\n",
    "        currentTags.append(tag) #one-hot encoding of tag\n",
    "        count_step+=1\n",
    "        \n",
    "    #flush\n",
    "    if not currentUtt == []:\n",
    "        count_seq+=1\n",
    "        x = np.asarray(currentUtt)\n",
    "        p = np.asarray(currentPOS)\n",
    "        y = np.asarray(currentTags)\n",
    "        seq.append(x)\n",
    "        pos_seq.append(p)\n",
    "        targets.append(y)\n",
    "    assert len(seq) == len(targets) == len(pos_seq)\n",
    "    #raw_input()\n",
    "    print \"loaded \" + str(len(seq)) + \" sequences\"\n",
    "    f.close()\n",
    "    return (seq,pos_seq,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_old():\n",
    "\n",
    "    theano.config.optimizer='None' #speeds things up marginally\n",
    "    # load the dataset\n",
    "    train_set, valid_set, test_set, valid_set_alltags, test_set_alltags, dic, train_dict = \\\n",
    "                                                            switchboardfold_old(fold=None, rpMid=False) \n",
    "    #adding train_dict as not all tags available in testing\n",
    "    #will not punish system for getting these wrong.\n",
    "    \n",
    "    print str(len(train_dict['labels2idx'].items())) + \" training classes\"\n",
    "    print str(len(dic['labels2idx'].items())) + \" testing classes\"\n",
    "    print str(len(dic['words2idx'].items())) + \" words in vocab\"\n",
    "    if not dic.get('pos2idx') == None:\n",
    "        print str(len(dic['pos2idx'].items())) + \" pos tags in vocab\"\n",
    "\n",
    "    idx2label_train = dict((k,v) for v,k in train_dict['labels2idx'].iteritems()) # first half (28) the same as the test\n",
    "    idx2label = dict((k,v) for v,k in dic['labels2idx'].iteritems())\n",
    "    idx2word  = dict((k,v) for v,k in dic['words2idx'].iteritems())\n",
    "    if not dic.get('pos2idx') == None:\n",
    "        idx2pos = dict((k,v) for v,k in dic['pos2idx'].iteritems())\n",
    "\n",
    "    #Now including pos tags\n",
    "    train_lex, train_pos, train_y = train_set\n",
    "    valid_lex, valid_pos, valid_y = valid_set\n",
    "    test_lex,  test_pos, test_y  = test_set\n",
    "    \n",
    "    #sets with ALL tags, i.e. those not in training:\n",
    "    valid_y_alltags = valid_set_alltags[-1] #always the last one\n",
    "    test_y_alltags = test_set_alltags[-1] #always the last one\n",
    "    \n",
    "    vocsize = len(dic['words2idx'].items())\n",
    "    #nclasses = len(dic['labels2idx'].items()) # actually smaller in reality, i.e. the below\n",
    "    nclasses = len(train_dict['labels2idx'].items())\n",
    "    nsentences = len(train_lex)\n",
    "    possize = None\n",
    "    if not dic.get('pos2idx') == None:\n",
    "        possize = len(idx2pos.items())\n",
    "    \n",
    "    nwords = len(list(itertools.chain(*train_y))) # TODO have added this\n",
    "    \n",
    "    print str(nsentences) + \" training sequences\"\n",
    "    print \"instantiating model\"\n",
    "    # instantiate the model\n",
    "    \n",
    "    # TODO shuffle_old([train_lex,train_pos,train_y], s['seed']) #shuffle training data\n",
    "    \n",
    "    s = {'win' : 2, 'bs' : 9}\n",
    "    # The new code trying to use theano more: converting into matrices with indices\n",
    "    mycorpus, myb_indices = corpusToIndexedMatrix_old(train_lex, s['win'], s['bs']) #window size across number of words deep, gets matrix too\n",
    "    mypos = corpusToIndexedMatrix_old(train_pos, s['win'], s['bs'])[0] # first column is the actual POS windows (which are indices to one hot vectors)\n",
    "    mylabels = list(itertools.chain(*train_y))\n",
    "    mylabels = numpy.asarray(mylabels, dtype='int32')\n",
    "    # Now see how they differ...\n",
    "    return mycorpus, mypos, myb_indices, mylabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "loaded 90509 sequences\n",
      "loading training data\n",
      "loaded 5717 sequences\n",
      "loading training data\n",
      "loaded 5942 sequences\n",
      "loading training data\n",
      "loaded 5717 sequences\n",
      "loading training data\n",
      "loaded 5942 sequences\n",
      "27 training classes\n",
      "50 testing classes\n",
      "9070 words in vocab\n",
      "127 pos tags in vocab\n",
      "90509 training sequences\n",
      "instantiating model\n"
     ]
    }
   ],
   "source": [
    "my_c, my_pos, my_indices, my_labels = load_old()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from deep_disfluency.tagger.deep_tagger import DeepDisfluencyTagger\n",
    "import numpy as np\n",
    "from deep_disfluency.utils.tools import dialogue_data_and_indices_from_matrix\n",
    "from deep_disfluency.load.load import load_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tagger\n",
      "Processing args from config file...\n",
      "Intializing model from args...\n",
      "Using the cpu\n",
      "Warning: not using GPU, might be a bit slow\n",
      "\tAdjust Theano config file ($HOME/.theanorc)\n",
      "loading tag to index maps...\n",
      "Initializing model of type elman ...\n",
      "Loading saved weights from ../../../deep_disfluency/experiments/021/epoch_40\n",
      "No POS tagger specified,loading default CRF switchboard one\n",
      "No timer specified, using default switchboard one\n",
      "Loading decoder...\n",
      "loading swbd_disf1_021 Markov model\n"
     ]
    }
   ],
   "source": [
    "disf = DeepDisfluencyTagger(\n",
    "    config_file=\"../../../deep_disfluency/experiments/experiment_configs.csv\",\n",
    "    config_number=21,\n",
    "    saved_model_dir=\"../../../deep_disfluency/experiments/021/epoch_40\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_dialogues_filepath = \"../../data/disfluency_detection/feature_matrices/train\"\n",
    "n_extra = 0\n",
    "utts_presegmented = True\n",
    "window_size = 2\n",
    "bs = 9\n",
    "tags = \"disf1_tags\"\n",
    "tag_to_index_map = load_tags(\"../../data/tag_representations/swbd_disf1_021_tags.csv\")\n",
    "train_matrices = [np.load(\n",
    "                            validation_dialogues_filepath + \"/\" + fp)\n",
    "                       for fp in os.listdir(\n",
    "                        validation_dialogues_filepath)]\n",
    "train_matrices = [dialogue_data_and_indices_from_matrix(\n",
    "                                  d_matrix,\n",
    "                                  n_extra,\n",
    "                                  pre_seg=utts_presegmented,\n",
    "                                  window_size=window_size,\n",
    "                                  bs=bs,\n",
    "                                  tag_rep=tags,\n",
    "                                  tag_to_idx_map=tag_to_index_map,\n",
    "                            in_utterances=utts_presegmented)\n",
    "                       for d_matrix in train_matrices\n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  -1,  708],\n",
       "        [ 708, 8343],\n",
       "        [8343, 3346],\n",
       "        ..., \n",
       "        [4363, 3519],\n",
       "        [3519, 5316],\n",
       "        [5316,  241]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrices[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
