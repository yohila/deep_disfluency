{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from theano import tensor as T\n",
    "from collections import OrderedDict\n",
    "#from test.test_iterlen import NoneLengthHint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We want to do something like the clockwork LSTM/RNN\n",
    "#Where acoustic data comes in continuously whilst the state of the word/pos input only\n",
    "#changes when specified.\n",
    "\n",
    "class Elman(object):\n",
    "    \n",
    "    def __init__(self, ne, de, nh, nc, cs, npos, update_embeddings=True):\n",
    "        '''\n",
    "        nh :: dimension of the hidden layer\n",
    "        nc :: number of classes\n",
    "        ne :: number of word embeddings in the vocabulary\n",
    "        de :: dimension of the word embeddings\n",
    "        cs :: word window context size \n",
    "        npos :: number of pos tags\n",
    "        '''\n",
    "        # parameters of the model\n",
    "        self.emb = theano.shared(0.2 * np.random.uniform(-1.0, 1.0,\\\n",
    "                   (ne+1, de)).astype(theano.config.floatX)) # add one for PADDING at the end\n",
    "        #=======================================================================\n",
    "        # self.Wx  = theano.shared(0.2 * np.random.uniform(-1.0, 1.0,\\\n",
    "        #            (de * cs, nh)).astype(theano.config.floatX))\n",
    "        #=======================================================================\n",
    "        self.Wx  = theano.shared(0.2 * np.random.uniform(-1.0, 1.0,\\\n",
    "                    ((de * cs)+(npos * cs), nh)).astype(theano.config.floatX))\n",
    "        self.Wh  = theano.shared(0.2 * np.random.uniform(-1.0, 1.0,\\\n",
    "                   (nh, nh)).astype(theano.config.floatX))\n",
    "        self.W   = theano.shared(0.2 * np.random.uniform(-1.0, 1.0,\\\n",
    "                   (nh, nc)).astype(theano.config.floatX))\n",
    "        self.bh  = theano.shared(np.zeros(nh, dtype=theano.config.floatX))\n",
    "        self.b   = theano.shared(np.zeros(nc, dtype=theano.config.floatX))\n",
    "        self.h0  = theano.shared(np.zeros(nh, dtype=theano.config.floatX))\n",
    "        #TODO bit of a hack, just using the eye function (diagonal 1s) for the POS, small in memory\n",
    "        self.pos = T.eye(npos,npos,0)\n",
    "        \n",
    "        #Weights for L1 and L2\n",
    "        self.L1_reg = 0.0\n",
    "        self.L2_reg = 0.00001\n",
    "\n",
    "\n",
    "        # bundle\n",
    "        self.params = [ self.Wx, self.Wh, self.W, self.bh, self.b, self.h0 ] #without embeddings updates\n",
    "        self.names  = ['Wx', 'Wh', 'W', 'bh', 'b', 'h0']\n",
    "        if update_embeddings:\n",
    "            self.params = [ self.emb, self.Wx, self.Wh, self.W, self.bh, self.b, self.h0 ]\n",
    "            self.names  = ['embeddings', 'Wx', 'Wh', 'W', 'bh', 'b', 'h0']\n",
    "        \n",
    "        self.idxs = T.imatrix() # as many columns as context window size/lines as words in the sentence\n",
    "    \n",
    "        self.pos_idxs = T.imatrix()\n",
    "        \n",
    "       \n",
    "\n",
    "        #TODO Old version no pos\n",
    "        #x = self.emb[self.idxs].reshape((self.idxs.shape[0], de*cs))\n",
    "        #TODO POS version, not just the embeddings, but with the POS window concatenated?\n",
    "        x = T.concatenate((self.emb[self.idxs].reshape((self.idxs.shape[0], de*cs)),\\\n",
    "                           self.pos[self.pos_idxs].reshape((self.pos_idxs.shape[0],npos*cs))), 1)\n",
    "        \n",
    "        self.y = T.iscalar('y') # label\n",
    "        #TODO for sentences\n",
    "        #self.y = T.ivector('y') #labels for whole sentence\n",
    "        \n",
    "        \n",
    "        #adding acoustic data scan function here\n",
    "        #this operates on a smaller time-scale, the hidden state, once computed\n",
    "        #will be used in conjunction with the 'slow' hidden state with the words when a new one comes in\n",
    "        #according to an index matrix above\n",
    "        \n",
    "        \n",
    "        def recurrence_2(x_t, h_tm1):\n",
    "            \"\"\"The second most outer loop, ('faster' than the most outer one) for acoustic input.\"\"\"\n",
    "            h_t = T.nnet.sigmoid(T.dot(x_t, self.Wx) + T.dot(h_tm1, self.Wh) + self.bh)\n",
    "            #s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
    "            return [h_t]\n",
    "        \n",
    "        [h_a], _ = theano.scan(fn=reccurence_2,\\\n",
    "                        sequences = acoustic_x, outputs_info=[self.h0_2, None],\\\n",
    "                        n_steps = x_acoustic.shape[0])\n",
    "        \n",
    "        \n",
    "        def recurrence(x_t, h_tm1):\n",
    "            h_t = T.nnet.sigmoid(T.dot(x_t, self.Wx) + T.dot(h_tm1, self.Wh) + self.bh)\n",
    "            s_t = T.nnet.softmax(T.dot(h_t, self.W) + self.b)\n",
    "            return [h_t, s_t]\n",
    "\n",
    "        [h, s], _ = theano.scan(fn=recurrence, \\\n",
    "            sequences=x, outputs_info=[self.h0, None], \\\n",
    "            n_steps=x.shape[0])\n",
    "        \n",
    "        \n",
    "\n",
    "        p_y_given_x_lastword = s[-1,0,:]\n",
    "        p_y_given_x_sentence = s[:,0,:]\n",
    "        y_pred = T.argmax(p_y_given_x_sentence, axis=1)\n",
    "        \n",
    "        #TODO adding this- zero one loss for the last word\n",
    "        y_pred_word = T.argmax(p_y_given_x_lastword)\n",
    "\n",
    "        #learning rate not hard coded as could decay\n",
    "        self.lr = T.scalar('lr')\n",
    "        \n",
    "        #Cost: standard nll loss\n",
    "        self.nll = -T.mean(T.log(p_y_given_x_lastword)[self.y])\n",
    "        \n",
    "        self.sentence_nll = -T.mean(T.log(p_y_given_x_sentence)\n",
    "                               [T.arange(x.shape[0]), self.y])\n",
    "        \n",
    "        \n",
    "        #theano functions #NB added POS\n",
    "        self.classify = theano.function(inputs=[self.idxs,self.pos_idxs], outputs=y_pred)\n",
    "        \n",
    "        #regularisation terms\n",
    "        # L1 norm ; one regularization option is to enforce L1 norm to\n",
    "        # be small\n",
    "        # if not using this set this to 0 to avoid unecessary computation\n",
    "        self.L1 = 0\n",
    "        #self.L1 = abs(self.Wh.sum()) +  abs(self.Wx.sum()) +  abs(self.W.sum()) +  abs(self.emb.sum())\\\n",
    "        #            +  abs(self.bh.sum()) + abs(self.b.sum()) + abs(self.h0.sum())\n",
    "\n",
    "        # square of L2 norm ; one regularization option is to enforce\n",
    "        # square of L2 norm to be small\n",
    "        self.L2_sqr =  (self.Wh ** 2).sum() + (self.Wx ** 2).sum() + (self.W ** 2).sum() +  (self.emb ** 2).sum()\\\n",
    "                        +  (self.bh ** 2).sum() +  (self.b ** 2).sum() + (self.h0 ** 2).sum()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #the gradients to compute\n",
    "        #gradients = T.grad( self.nll, self.params )\n",
    "        #TODO am changing to below\n",
    "        \n",
    "        # costs for single label per input sequence\n",
    "        cost = self.nll \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "        gradients = T.grad( cost, self.params )\n",
    "        \n",
    "        self.updates = OrderedDict(( p, p-self.lr*g ) for p, g in zip( self.params , gradients))\n",
    "        \n",
    "        #costs for multiple labels (one for each in the input)\n",
    "        sentence_cost = self.sentence_nll \\\n",
    "            + self.L1_reg * self.L1 \\\n",
    "            + self.L2_reg * self.L2_sqr\n",
    "        sentence_gradients = T.grad(sentence_cost, self.params)\n",
    "        \n",
    "        self.sentence_updates = OrderedDict((p, p - self.lr*g)\n",
    "                                       for p, g in\n",
    "                                       zip(self.params, sentence_gradients))\n",
    "        \n",
    "        \n",
    "        # theano functions #NB added POS\n",
    "        self.soft_max = theano.function(inputs=[self.idxs, self.pos_idxs], outputs=p_y_given_x_sentence) #simply outputs the soft_max distribution for each word in utterance\n",
    "\n",
    "\n",
    "        #=======================================================================\n",
    "        # #ORIGINAL CODE\n",
    "        # self.train = theano.function( inputs  = [self.idxs, self.y, lr],\n",
    "        #                               outputs = nll,\n",
    "        #                               updates = self.updates )\n",
    "        #=======================================================================\n",
    "        \n",
    "        \n",
    "        self.normalize = theano.function( inputs = [],\n",
    "                         updates = {self.emb:\\\n",
    "                         self.emb/T.sqrt((self.emb**2).sum(axis=1)).dimshuffle(0,'x')})\n",
    "\n",
    "    def fit(self, my_seq, my_indices, my_labels, lr, nw, pos=None, sentence=False):\n",
    "        \"\"\"The main training function over the corpus indexing into my_indices\"\"\"\n",
    "        tic = time.time()\n",
    "        corpus = self.shared_dataset(my_seq) #loads data set as a shared variable\n",
    "        labels = self.shared_dataset(my_labels)\n",
    "        if not pos == None:\n",
    "            pos = self.shared_dataset(pos)\n",
    "        #TODO new effort to index the shared vars\n",
    "        batchstart = T.iscalar('batchstart')\n",
    "        batchstop = T.iscalar('batchstop')\n",
    "        if sentence == True:\n",
    "            cost = self.sentence_nll\n",
    "            updates = self.sentence_updates\n",
    "        else:    \n",
    "            cost = self.nll\n",
    "            updates = self.updates\n",
    "        if pos == None:\n",
    "            self.train_by_index = theano.function( inputs  = [batchstart, batchstop,self.lr],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates, \n",
    "                                      givens={self.idxs : corpus[batchstart:batchstop+1],\n",
    "                                              self.y : labels[batchstop]},\n",
    "                                      on_unused_input='warn')  \n",
    "            #TODO have changed  self.y : labels[batchstop]}, \n",
    "        else:\n",
    "            self.train_by_index = theano.function( inputs  = [batchstart, batchstop,self.lr],\n",
    "                                      outputs = cost,\n",
    "                                      updates = updates, \n",
    "                                      givens={self.idxs : corpus[batchstart:batchstop+1],\n",
    "                                              self.pos_idxs : pos[batchstart:batchstop+1],\n",
    "                                              self.y : labels[batchstop]},\n",
    "                                      on_unused_input='warn') \n",
    "            #TODO have changed  self.y : labels[batchstop]},   \n",
    "        train_loss = 0.0\n",
    "        laststop = 0\n",
    "        i= 0\n",
    "        for start,stop in my_indices:\n",
    "            laststop = stop\n",
    "            i+=1 #TODO for testing\n",
    "            if i> 50: break\n",
    "            x = self.train_by_index(start,stop,lr)\n",
    "            train_loss+=x\n",
    "            #print i\n",
    "            self.normalize()\n",
    "            #if stop % 6500 == 0 and stop>0:\n",
    "            print '[learning] >> %2.2f%%'%((stop+1)*100./nw),'completed in %.2f (sec) <<\\r'%(time.time()-tic),\n",
    "            sys.stdout.flush()\n",
    "        print \"train_loss (may include reg)\"\n",
    "        print train_loss/float(laststop)\n",
    "        return train_loss/float(laststop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n",
      "  from scan_perform.scan_perform import *\n"
     ]
    }
   ],
   "source": [
    "e = Elman(20, 10, 10, 5, 2, 10, update_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some fake data\n",
    "x = 20 * [0,]\n",
    "x[8] = 1\n",
    "xpos = 10 * [0,]\n",
    "xpos[2] =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print x, xpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 3]] [[1 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.17346836,  0.20731865,  0.19915406,  0.22270885,  0.19735009]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray([[3,3]]).astype('int32')\n",
    "xpos = np.asarray([[1,2]]).astype('int32')\n",
    "print x,xpos\n",
    "e.soft_max(x,xpos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named ipdb",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6f043c260ba5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNestedLoopTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_filters_per_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named ipdb"
     ]
    }
   ],
   "source": [
    "import ipdb\n",
    "\n",
    "class NestedLoopTest(object):\n",
    "\n",
    "    def __init__(self, num_filters_per_dim):\n",
    "        \"\"\"\n",
    "        :param num_filters_per_dim: number of filters per dimension\n",
    "        \"\"\"\n",
    "        self.num_filters_per_dim=num_filters_per_dim\n",
    "\n",
    "\n",
    "    def mult_for_loop(self, weights, img):\n",
    "        tmp=0\n",
    "\n",
    "        for j in xrange(self.num_filters_per_dim):\n",
    "            for k in xrange(self.num_filters_per_dim):\n",
    "                lindex=k*self.num_filters_per_dim+j\n",
    "                tmp=tmp+weights[lindex]*img\n",
    "\n",
    "        return tmp\n",
    "\n",
    "\n",
    "    def mult_scan_loop(self, weights, img):\n",
    "\n",
    "        def inner_loop(k,r,j):\n",
    "            \"\"\"\n",
    "            :type k: sequences\n",
    "            :param k: indices of the inner loop\n",
    "\n",
    "            :type r: outputs_info\n",
    "            :param r: temporary results of the inner loop\n",
    "\n",
    "            :type j: non_sequences\n",
    "            :param j: index of the outer loop\n",
    "            \"\"\"\n",
    "\n",
    "            lindex=k*self.num_filters_per_dim+j\n",
    "\n",
    "            r=r+weights[lindex]*img\n",
    "        \n",
    "            return r\n",
    "        # inner_loop\n",
    "\n",
    "        def outer_loop(j,r):\n",
    "            \"\"\"\n",
    "            :type j: sequences\n",
    "            :param j: indices of the outer loop\n",
    "\n",
    "            :type r: outputs_info\n",
    "            :param r: temporary results of the outer loop\n",
    "            \"\"\"\n",
    "            results,_=theano.scan(\n",
    "                fn=inner_loop, \n",
    "                sequences=[T.arange(self.num_filters_per_dim)],\n",
    "                outputs_info=r,\n",
    "                non_sequences=j)\n",
    "            # we discard other computations\n",
    "            results=results[-1]\n",
    "\n",
    "            r = r + results\n",
    "\n",
    "            return r\n",
    "        # outer_loop\n",
    "\n",
    "        r=T.zeros_like(img, dtype=theano.config.floatX)\n",
    "\n",
    "        results,_=theano.scan(\n",
    "            fn=outer_loop,\n",
    "            sequences=[T.arange(self.num_filters_per_dim)],\n",
    "            outputs_info=r)\n",
    "\n",
    "        # we discard other computations\n",
    "        results=results[-1]\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "def do_test(images, weights, my_op):\n",
    "    images = theano.shared(images)\n",
    "    weights = theano.shared(weights,broadcastable=(False,True,True))\n",
    "\n",
    "    mweights = T.fvector('weights')\n",
    "    mimage = T.fmatrix('image')\n",
    "    index = T.lscalar('image index')\n",
    "\n",
    "    full_conv_apply = theano.function([index], my_op(weights,mimage),\n",
    "        givens={mimage:images[index]})\n",
    "\n",
    "    conv_out = np.zeros_like(images.get_value())\n",
    "\n",
    "    for image_no in xrange(NUM_IMAGES):\n",
    "        conv_out[image_no] = full_conv_apply(image_no)\n",
    "\n",
    "    return conv_out\n",
    "\n",
    "\n",
    "def create_random_images(num_images,nrows,ncols):\n",
    "    my_images=np.random.randint(low=0,high=2,\n",
    "        size=(num_images,nrows,ncols)).astype(theano.config.floatX)\n",
    "\n",
    "    return my_images\n",
    "\n",
    "\n",
    "def create_random_weights(num_basis):\n",
    "    weights = np.random.randn(num_basis,1,1).astype(theano.config.floatX)\n",
    "\n",
    "    return weights\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    NROWS=10\n",
    "    NCOLS=10\n",
    "    NUM_IMAGES=10\n",
    "    NUM_BASIS_PER_DIM=7\n",
    "\n",
    "    images = create_random_images(NUM_IMAGES, NROWS, NCOLS)\n",
    "    weights = create_random_weights(np.square(NUM_BASIS_PER_DIM))\n",
    "\n",
    "    nestedLoop=NestedLoopTest(NUM_BASIS_PER_DIM)\n",
    "\n",
    "    my_op1=nestedLoop.mult_for_loop\n",
    "    out1 = do_test(images, weights, my_op1)\n",
    "\n",
    "    my_op2=nestedLoop.mult_scan_loop\n",
    "    out2 = do_test(images, weights, my_op2)\n",
    "\n",
    "    print out1[0] == out2[0]\n",
    "\n",
    "    ipdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now for joint prob distributions\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self, input, n_in, n_outs):\n",
    "        \"\"\" Initialize the parameters of the logistic regression\n",
    "\n",
    "        :type n_outs: list of int\n",
    "        :param n_outs: number of output units in each group\n",
    "\n",
    "        \"\"\"\n",
    "        n_out = numpy.sum(n_outs)\n",
    "        n_groups = len(n_outs)\n",
    "        self.n_groups = n_groups\n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out)\n",
    "        self.W = theano.shared(value=numpy.zeros((n_in,n_out), dtype = theano.config.floatX),\n",
    "                                name='W')\n",
    "        # initialize the baises b as a vector of n_out 0s\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,), dtype = theano.config.floatX),\n",
    "                               name='b')\n",
    "\n",
    "        self.h = T.dot(input, self.W) + self.b\n",
    "        self.p_y_given_x = []\n",
    "        self.y_pred = []\n",
    "        t = 0\n",
    "        for idx in xrange(n_groups):\n",
    "            p_y_given_x = T.nnet.softmax( self.h[t:t+n_outs[idx]])\n",
    "            y_pred = T.argmax( p_y_given_x, axis = 1)\n",
    "            t+= n_outs[idx]\n",
    "            self.p_y_given_x.append( p_y_given_x)\n",
    "            self.y_pred.append( y_pred )\n",
    "\n",
    "        # parameters of the model\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "    def negative_log_likelihood(self, ys):\n",
    "        cost = -T.mean(T.log( self.p_y_given_x[0])[T.arange(ys[0].shape[0]),ys[0]])\n",
    "        for idx in xrange(1, self.n_groups):\n",
    "            cost += -T.mean(T.log( self.p_y_given_x[idx])[T.arange(ys[idx].shape[0]),ys[idx]])\n",
    "\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def errors(self, ys):\n",
    "        errs = []\n",
    "        for idx in xrange(self.n_groups):\n",
    "            if ys[idx].ndim != self.y_pred[idx].ndim:\n",
    "                raise TypeError('y should have the same shape as self.y_pred',\n",
    "                    ('y', ys[idx].type, 'y_pred', self.y_pred[idx].type))\n",
    "            # check if y is of the correct datatype\n",
    "            if ys[idx].dtype.startswith('int'):\n",
    "                # the T.neq operator returns a vector of 0s and 1s, where 1\n",
    "                # represents a mistake in prediction\n",
    "                errs.append( T.mean(T.neq(self.y_pred[idx], ys[idx])))\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        return errs\n",
    "\n",
    "class multiple():\n",
    "    CARD_IDX =np.cumsum( [3, 2])\n",
    "    for i in range(len(CARD_IDX)):\n",
    "        if i==0:\n",
    "            start_idx=0\n",
    "        else:\n",
    "            start_idx=CARD_IDX[i-1]\n",
    "\n",
    "        end_idx = CARD_IDX[i]            \n",
    "        idx_range = range(start_idx,end_idx)\n",
    "        T.dot(input, self.W[:,idx_range]) + self.b[idx_range]\n",
    "\n",
    "        temp = T.set_subtensor(temp[:,idx_range],\\\n",
    "                               T.nnet.softmax(T.dot(input, self.W[:,idx_range]) + self.b[idx_range]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This tutorial introduces logistic regression using Theano and stochastic\n",
    "gradient descent.\n",
    "\n",
    "Logistic regression is a probabilistic, linear classifier. It is parametrized\n",
    "by a weight matrix :math:`W` and a bias vector :math:`b`. Classification is\n",
    "done by projecting data points onto a set of hyperplanes, the distance to\n",
    "which is used to determine a class membership probability.\n",
    "\n",
    "Mathematically, this can be written as:\n",
    "\n",
    ".. math::\n",
    "  P(Y=i|x, W,b) &= softmax_i(W x + b) \\\\\n",
    "                &= \\frac {e^{W_i x + b_i}} {\\sum_j e^{W_j x + b_j}}\n",
    "\n",
    "\n",
    "The output of the model or prediction is then done by taking the argmax of\n",
    "the vector whose i'th element is P(Y=i|x).\n",
    "\n",
    ".. math::\n",
    "\n",
    "  y_{pred} = argmax_i P(Y=i|x,W,b)\n",
    "\n",
    "\n",
    "This tutorial presents a stochastic gradient descent optimization method\n",
    "suitable for large datasets.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "    - textbooks: \"Pattern Recognition and Machine Learning\" -\n",
    "                 Christopher M. Bishop, section 4.3.2\n",
    "\n",
    "\"\"\"\n",
    "__docformat__ = 'restructedtext en'\n",
    "\n",
    "import cPickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "class GroupedLogisticRegression(object): \n",
    "    \"\"\"Multi-class Logistic Regression Class \n",
    "    The logistic regression is fully described by a weight matrix :math:`W`\n",
    "    and bias vector :math:`b`. Classification is done by projecting \n",
    "    data points onto a set of hyperplanes, the distance to which is used \n",
    "    to determine a class membership probability. \n",
    "    \"\"\" \n",
    "\n",
    "    def __init__(self, input, n_in, n_outs): \n",
    "        \"\"\" Initialize the parameters of the logistic regression \n",
    "\n",
    "        :type n_outs: list of int \n",
    "        :param n_outs: number of output units in each group \n",
    "\n",
    "        \"\"\" \n",
    "        n_out = numpy.sum(n_outs) \n",
    "        n_groups = len(n_outs) \n",
    "        self.n_groups = n_groups \n",
    "        # initialize with 0 the weights W as a matrix of shape (n_in, n_out) \n",
    "        self.W = theano.shared(value = numpy.zeros((n_in,n_out), \n",
    "                                dtype = theano.config.floatX), \n",
    "                                name = 'W') \n",
    "        # initialize the baises b as a vector of n_out 0s \n",
    "        self.b = theano.shared(value = numpy.zeros((n_out,), \n",
    "                               dtype = theano.config.floatX), \n",
    "                               name = 'b') \n",
    "\n",
    "        self.h = T.dot(input, self.W) + self.b \n",
    "        self.p_y_given_x = [] \n",
    "        self.y_pred = [] \n",
    "        t = 0 \n",
    "        for idx in xrange(n_groups): \n",
    "            p_y_given_x = T.nnet.softmax( self.h[:,t:t+n_outs[idx]] ) \n",
    "            y_pred = T.argmax( p_y_given_x, axis = 1 ) \n",
    "            t += n_outs[idx] \n",
    "            self.p_y_given_x.append( p_y_given_x ) \n",
    "            self.y_pred.append( y_pred ) \n",
    "\n",
    "        # parameters of the model \n",
    "        self.params = [self.W, self.b] \n",
    "\n",
    "    def negative_log_likelihood(self, ys): \n",
    "        \"\"\"Return the mean of the negative log-likelihood of the \n",
    "        prediction of this model under a given target distribution. \n",
    "\n",
    "        .. math:: \n",
    "            \\frac{1}{|\\mathcal{D}|} \\mathcal{L} (\\theta=\\{W,b\\}, \\mathcal{D}) = \n",
    "            \\frac{1}{|\\mathcal{D}|} \\sum_{i=0}^{|\\mathcal{D}|} \\log(P(Y=y^{(i)}|x^{(i)}, W,b)) \\\\ \n",
    "                \\ell (\\theta=\\{W,b\\}, \\mathcal{D}) \n",
    "\n",
    "        :type y: theano.tensor.TensorType \n",
    "        :param y: corresponds to a vector that gives for each example \n",
    "                the correct label \n",
    "\n",
    "        Note: we use the mean instead of the sum so that \n",
    "              the learning rate is less dependent on the batch size \n",
    "        \"\"\" \n",
    "        # y.shape[0] is (symbolically) the number of rows in y, i.e., \n",
    "        # number of examples (call it n) in the minibatch \n",
    "        # T.arange(y.shape[0]) is a symbolic vector which will contain [0,1,2,... n-1] \n",
    "        # T.log(self.p_y_given_x) is a matrix of Log-Probabilities \n",
    "        # (call it LP) with one row per example and one column per class \n",
    "        # LP[T.arange(y.shape[0]),y] is a vector v containing \n",
    "        # [LP[0,y[0]], LP[1,y[1]], LP[2,y[2]], ..., LP[n-1,y[n-1]]] \n",
    "        # and T.mean(LP[T.arange(y.shape[0]),y]) is the mean (across \n",
    "        # inibatch examples) of the elements in v, \n",
    "        # i.e., the mean log-likelihood across the minibatch. \n",
    "        cost = -T.mean(T.log(self.p_y_given_x[0])[T.arange(ys[:,0].shape[0]), ys[:,0]]) \n",
    "        for idx in xrange(1, self.n_groups): \n",
    "            cost += -T.mean(T.log(self.p_y_given_x[idx])[T.arange(ys[:,idx].shape[0]), ys[:,idx]]) \n",
    "\n",
    "        return cost \n",
    "\n",
    "    def errors(self, ys): \n",
    "        errs = [] \n",
    "        for idx in xrange(self.n_groups): \n",
    "            if ys[:,idx].ndim != self.y_pred[idx].ndim: \n",
    "                raise TypeError('y should have the same shape as self.y_pred', \n",
    "                    ('y', ys[:,idx].type, 'y_pred', self.y_pred[idx].type)) \n",
    "            # check if y is of the correct datatype \n",
    "            if ys[:,idx].dtype.startswith('int'): \n",
    "                # the T.neq operator returns a vector of 0s and 1s, where 1 \n",
    "                # represents a mistake in prediction \n",
    "                errs.append( T.mean(T.neq(self.y_pred[idx], ys[:,idx]))) \n",
    "            else: \n",
    "                raise NotImplementedError() \n",
    "        return errs\n",
    "\n",
    "\n",
    "def load_data(dataset): \n",
    "    ''' Loads the dataset \n",
    "\n",
    "    :type dataset: string \n",
    "    :param dataset: the path to the dataset (here MNIST) \n",
    "    ''' \n",
    "    ############# \n",
    "    # LOAD DATA # \n",
    "    ############# \n",
    "    print '... loading data (grouped)' \n",
    "\n",
    "    # Load the dataset \n",
    "    f = gzip.open(dataset,'rb') \n",
    "    train_set, valid_set, test_set = cPickle.load(f) \n",
    "    f.close() \n",
    "\n",
    "\n",
    "    def shared_dataset(data_xy): \n",
    "        \"\"\" Function that loads the dataset into shared variables \n",
    "        The reason we store our dataset in shared variables is to allow \n",
    "        Theano to copy it into the GPU memory (when code is run on GPU). \n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime \n",
    "        is needed (the default behaviour if the data is not in a shared \n",
    "        variable) would lead to a large decrease in performance. \n",
    "        \"\"\" \n",
    "        data_x, data_y = data_xy \n",
    "        data_y = data_y.reshape(data_y.shape[0], 1) \n",
    "        print data_x.shape \n",
    "        print data_y.shape \n",
    "        shared_x = theano.shared(numpy.asarray(data_x, \n",
    "                                    dtype=theano.config.floatX)) \n",
    "        shared_y = theano.shared(numpy.asarray(data_y, \n",
    "                                    dtype=theano.config.floatX)) \n",
    "        # When storing data on the GPU it has to be stored as floats \n",
    "        # therefore we will store the labels as ``floatX`` as well \n",
    "        # (``shared_y`` does exactly that). But during our computations \n",
    "        # we need them as ints (we use labels as index, and if they are \n",
    "        # floats it doesn't make sense) therefore instead of returning \n",
    "        # ``shared_y`` we will have to cast it to int. This little hack \n",
    "        # lets ous get around this issue \n",
    "        return shared_x, T.cast(shared_y, 'int32') \n",
    "\n",
    "    test_set_x,  test_set_y  = shared_dataset(test_set) \n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set) \n",
    "    train_set_x, train_set_y = shared_dataset(train_set) \n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x,valid_set_y), (test_set_x, test_set_y)] \n",
    "    return rval \n",
    "\n",
    "\n",
    "def sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,\n",
    "                           dataset='mnist.pkl.gz',\n",
    "                           batch_size=600):\n",
    "    \"\"\"\n",
    "    Demonstrate stochastic gradient descent optimization of a log-linear\n",
    "    model\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used (factor for the stochastic\n",
    "                          gradient)\n",
    "\n",
    "    :type n_epochs: int\n",
    "    :param n_epochs: maximal number of epochs to run the optimizer\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: the path of the MNIST dataset file from\n",
    "                 http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "    n_test_batches = test_set_x.get_value(borrow=True).shape[0] / batch_size\n",
    "\n",
    "    ######################\n",
    "    # BUILD ACTUAL MODEL #\n",
    "    ######################\n",
    "    print '... building the model'\n",
    "\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "\n",
    "    # generate symbolic variables for input (x and y represent a\n",
    "    # minibatch)\n",
    "    x = T.matrix('x')  # data, presented as rasterized images\n",
    "    y = T.imatrix('y')  # labels, presented as 1D vector of [int] labels\n",
    "\n",
    "    # construct the logistic regression class\n",
    "    # Each MNIST image has size 28*28\n",
    "    classifier = GroupedLogisticRegression(input=x, n_in=28 * 28, n_outs=[10])\n",
    "\n",
    "    # the cost we minimize during training is the negative log likelihood of\n",
    "    # the model in symbolic format\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    # compiling a Theano function that computes the mistakes that are made by\n",
    "    # the model on a minibatch\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # compute the gradient of cost with respect to theta = (W,b)\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "\n",
    "    # start-snippet-3\n",
    "    # specify how to update the parameters of the model as a list of\n",
    "    # (variable, update expression) pairs.\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "    # compiling a Theano function `train_model` that returns the cost, but in\n",
    "    # the same time updates the parameter of the model based on the rules\n",
    "    # defined in `updates`\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "    # end-snippet-3\n",
    "\n",
    "    ###############\n",
    "    # TRAIN MODEL #\n",
    "    ###############\n",
    "    print '... training the model'\n",
    "    # early-stopping parameters\n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is\n",
    "                                  # found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is\n",
    "                                  # considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "                                  # go through this many\n",
    "                                  # minibatche before checking the network\n",
    "                                  # on the validation set; in this case we\n",
    "                                  # check every epoch\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = time.clock()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            # iteration number\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [validate_model(i)\n",
    "                                     for i in xrange(n_valid_batches)]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "\n",
    "                print(\n",
    "                    'epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                    (\n",
    "                        epoch,\n",
    "                        minibatch_index + 1,\n",
    "                        n_train_batches,\n",
    "                        this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss *  \\\n",
    "                       improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    # test it on the test set\n",
    "\n",
    "                    test_losses = [test_model(i)\n",
    "                                   for i in xrange(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "\n",
    "                    print(\n",
    "                        (\n",
    "                            '     epoch %i, minibatch %i/%i, test error of'\n",
    "                            ' best model %f %%'\n",
    "                        ) %\n",
    "                        (\n",
    "                            epoch,\n",
    "                            minibatch_index + 1,\n",
    "                            n_train_batches,\n",
    "                            test_score * 100.\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = time.clock()\n",
    "    print(\n",
    "        (\n",
    "            'Optimization complete with best validation score of %f %%,'\n",
    "            'with test performance %f %%'\n",
    "        )\n",
    "        % (best_validation_loss * 100., test_score * 100.)\n",
    "    )\n",
    "    print 'The code run for %d epochs, with %f epochs/sec' % (\n",
    "        epoch, 1. * epoch / (end_time - start_time))\n",
    "    print >> sys.stderr, ('The code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.1fs' % ((end_time - start_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
